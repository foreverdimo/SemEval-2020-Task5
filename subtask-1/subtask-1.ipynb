{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNLSTM, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model,Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "tqdm.pandas()\n",
    "np.random.seed(2020)\n",
    "\n",
    "embedding_dims = 300 \n",
    "max_features = 20000\n",
    "maxlen = 200\n",
    "test_percent = 0.1 \t# 0.2 for testing\n",
    "data_extra = True   # Use extra training-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Read data...\n",
      "File: subtask-1/train-extra.csv\n",
      "Train shape :  (16551, 4)\n",
      "Test shape :  (7000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Load the train data \n",
    "print(\">> Read data...\")\n",
    "if data_extra:\n",
    "    train_path = \"subtask-1/train-extra.csv\"\n",
    "else:\n",
    "    train_path = \"subtask-1/train.csv\"\n",
    "test_path = \"subtask-1/subtask1_test.csv\"\n",
    "train = pd.read_csv(train_path, encoding = 'utf-8')\n",
    "test = pd.read_csv(test_path, encoding = 'utf-8')\n",
    "\n",
    "print(\"File: %s\" % train_path)\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Read data...\n",
      "File: subtask-1/train-extra.csv\n",
      "Train shape :  (16551, 4)\n",
      "Test shape :  (7000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Load the pre-trained embedding\n",
    "## To use the pretrained embedding, you need to download the embedding file: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
    "## Unzip and place it in the directory of this project\n",
    "\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2v1ec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "embeddings = np.stack(list(embeddings_index[word] for word in embeddings_index.vocab))\n",
    "\n",
    "print(\"Google-news Doc2Vec Word Embeddings's shape:\")\n",
    "print(embeddings.shape)\n",
    "\n",
    "emb_mean, emb_std = embeddings.mean(), embeddings.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data pre-processing\n",
    "\n",
    "train_df, validation_df = model_selection.train_test_split(train, test_size= test_percent, random_state = 2020)\n",
    "\n",
    "## fill up the missing values\n",
    "train_X = train_df[\"sentence\"].fillna(\"_na_\").values\n",
    "validation_X = validation_df[\"sentence\"].fillna(\"_na_\").values\n",
    "test_X = test[\"sentence\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "validation_X = tokenizer.texts_to_sequences(validation_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "## Pad the sentences\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "validation_X = pad_sequences(validation_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_Y = train_df['gold_label'].values\n",
    "validation_Y = validation_df['gold_label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_matrix's shape:\n",
      "(20000, 300)\n"
     ]
    }
   ],
   "source": [
    "## Get processed pre-trained embedding matrix\n",
    " \n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embeddings_matrix = np.random.normal(emb_mean, emb_std, (max_features, embedding_dims))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    \n",
    "    if index >= max_features:\n",
    "        continue\n",
    "    try:\n",
    "        embeddings_vector = embeddings_index[word]\n",
    "        embeddings_matrix[index] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "print(\"embeddings_matrix's shape:\")    \n",
    "print(embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 200, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 6,188,961\n",
      "Trainable params: 188,961\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Build Biodirectional-LSTM Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, embeddings_initializer = initializers.Constant(embeddings_matrix), input_length = maxlen, trainable = False))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation = \"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14895 samples, validate on 1656 samples\n",
      "Epoch 1/15\n",
      "14895/14895 [==============================] - 14s 914us/step - loss: 0.5786 - accuracy: 0.6951 - val_loss: 0.4747 - val_accuracy: 0.7615\n",
      "Epoch 2/15\n",
      "14895/14895 [==============================] - 13s 901us/step - loss: 0.4195 - accuracy: 0.8188 - val_loss: 0.3800 - val_accuracy: 0.8364\n",
      "Epoch 3/15\n",
      "14895/14895 [==============================] - 14s 909us/step - loss: 0.3500 - accuracy: 0.8495 - val_loss: 0.3342 - val_accuracy: 0.8472\n",
      "Epoch 4/15\n",
      "14895/14895 [==============================] - 13s 887us/step - loss: 0.3079 - accuracy: 0.8698 - val_loss: 0.3186 - val_accuracy: 0.8611\n",
      "Epoch 5/15\n",
      "14895/14895 [==============================] - 13s 882us/step - loss: 0.2816 - accuracy: 0.8831 - val_loss: 0.3024 - val_accuracy: 0.8653\n",
      "Epoch 6/15\n",
      "14895/14895 [==============================] - 13s 887us/step - loss: 0.2542 - accuracy: 0.8964 - val_loss: 0.2722 - val_accuracy: 0.8877\n",
      "Epoch 7/15\n",
      "14895/14895 [==============================] - 13s 888us/step - loss: 0.2311 - accuracy: 0.9069 - val_loss: 0.2676 - val_accuracy: 0.8889\n",
      "Epoch 8/15\n",
      "14895/14895 [==============================] - 13s 899us/step - loss: 0.2084 - accuracy: 0.9186 - val_loss: 0.2756 - val_accuracy: 0.8883\n",
      "Epoch 9/15\n",
      "14895/14895 [==============================] - 14s 926us/step - loss: 0.1961 - accuracy: 0.9244 - val_loss: 0.2567 - val_accuracy: 0.8937\n",
      "Epoch 10/15\n",
      "14895/14895 [==============================] - 14s 913us/step - loss: 0.1636 - accuracy: 0.9409 - val_loss: 0.2667 - val_accuracy: 0.8949\n",
      "Epoch 11/15\n",
      "14895/14895 [==============================] - 13s 905us/step - loss: 0.1469 - accuracy: 0.9478 - val_loss: 0.2702 - val_accuracy: 0.8937\n",
      "Epoch 12/15\n",
      "14895/14895 [==============================] - 13s 890us/step - loss: 0.1281 - accuracy: 0.9554 - val_loss: 0.2964 - val_accuracy: 0.8961\n",
      "Epoch 13/15\n",
      "14895/14895 [==============================] - 13s 901us/step - loss: 0.1197 - accuracy: 0.9567 - val_loss: 0.2808 - val_accuracy: 0.8943\n",
      "Epoch 14/15\n",
      "14895/14895 [==============================] - 13s 905us/step - loss: 0.0955 - accuracy: 0.9675 - val_loss: 0.2909 - val_accuracy: 0.8961\n",
      "Epoch 15/15\n",
      "14895/14895 [==============================] - 14s 908us/step - loss: 0.0830 - accuracy: 0.9742 - val_loss: 0.3092 - val_accuracy: 0.8992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22241bede08>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model\n",
    "model.fit(train_X, train_Y, batch_size = 256, epochs = 15, validation_data = (validation_X, validation_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.8991546\n"
     ]
    }
   ],
   "source": [
    "_ , acc = model.test_on_batch(validation_X, validation_Y, sample_weight=None)\n",
    "print(\"Test accuracy : \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1656/1656 [==============================] - 0s 217us/step\n",
      "The F1_score is :0.8357915437561455\n"
     ]
    }
   ],
   "source": [
    "## Evaluate The Model\n",
    "Y_pred = model.predict([validation_X],batch_size = 256, verbose = 1)\n",
    "\n",
    "F1_score = f1_score(validation_Y, (Y_pred>0.5).astype(int))\n",
    "\n",
    "print(\"The F1_score is :\" + str(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
